<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Graphs are everywhere | nutorbit</title>
<meta name="keywords" content="graph neural network">
<meta name="description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge.">
<meta name="author" content="Nut Chukamphaeng">
<link rel="canonical" href="https://nutorbit.github.io/posts/graph_neural_network/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://nutorbit.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nutorbit.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nutorbit.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nutorbit.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nutorbit.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
    referrerpolicy="no-referrer">

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "\\[", right: "\\]", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
      ],
    });
  });
</script>
<meta property="og:title" content="Graphs are everywhere" />
<meta property="og:description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nutorbit.github.io/posts/graph_neural_network/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-08-13T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Graphs are everywhere"/>
<meta name="twitter:description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nutorbit.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Graphs are everywhere",
      "item": "https://nutorbit.github.io/posts/graph_neural_network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Graphs are everywhere",
  "name": "Graphs are everywhere",
  "description": "If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge.",
  "keywords": [
    "graph neural network"
  ],
  "articleBody": "If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge. For example, to classify the nodes in a social network, we need to extract features from the nodes and edges. Then, we need to design a model that can learn from the extracted features. Consequently, the model and data preprocessing pipeline are very specific to the task.\nNowdays, with the rise of deep learning, we can learn from graph structure without requiring a lot of domain knowledge. That’s where Graph Neural Networks (GNNs) come in.\nGraph Neural Network (GNN) is a neural network that can learn from graph structure. It provides a framework to learn from graph structure without requiring a lot of domain knowledge. Moreover, it can be applied to many tasks such as classifying nodes, predicting the connection between nodes, or predicting the graph structure.\nTo give you an idea of what GNN can do, here are some examples of applications:\nClassifying nodes: predict the type of a person in a social network (student, professor, etc.) or predict the atom type in a molecule.\nPredicting the connection between nodes: predict the friendship between two people in a social network.\nPredicting the graph structure: predict the type of a molecule from its structure.\nExamples of tasks that can be solved with GNNs\nIn this post, I will give you an overview of GNNs. Also, explaining in detail the different components of a GNN and how it works theoretically. This post will contain a lot of math. Please bear with me. Now, let’s get started!\nWhat is a graph? Graph is a data structure that consists of nodes and edges. The edges connect the nodes. Mathematically, a graph can be represented as a tuple $(V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. A graph can be directed or undirected. For simplicity, let’s focus on undirected graphs.\nExample of a graph with 4 nodes and 4 edges\nIn the above figure, the nodes are represented as circles ($V \\in {1, 2, 3, 4}$) and the edges are represented as blue lines ($E \\in {(1, 2), (1, 4), (2, 3), (2, 4)}$).\nAnother way to represent a graph is with an adjacency matrix. An adjacency matrix is a matrix $A$ of size $|V| \\times |V|$ where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$ and $A_{ij} = 0$ otherwise. For example, the adjacency matrix of the above graph is:\n$$ A = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 1 \u0026 1 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 0 \u0026 0 \\\\ \\end{bmatrix} $$ These two representations that I mentioned above are the most common and easy ways to represent a graph. Imagine if you want to add more information to the graph such as node information or edge information, this makes the graph representation more complicated, and it will be hard to work with.\nWhat is a Graph Neural Network? Now that we know what a graph is, it’s time to talk about the way to work with graph structure. I will assume that you are familiar with neural networks. If you are not, I recommend you to read this post first.\nA Graph Neural Network (GNN) is one type of neural network that can learn from graph structure to solve a task by aggregating information from the neighbors of each node. One of the well-known GNNs is the Message Passing Neural Network (MPNN) proposed by Gilmer et al. in this paper.\nTo make this post easier to follow, let’s focus on the task of classifying nodes.\nMessage Passing Neural Network The Message Passing Neural Network (MPNN) consists of three steps: Initialization, Message Passing, and Transformation. The initialization step is used to initialize the hidden state of each node (also called node embedding). The message passing step is used to aggregate information from the neighbors of each node. The transformation step is used to transform the hidden state of each node to a new hidden state.\nThe following equation shows how to update the hidden state of each node at time $t+1$:\n$$ \\begin{align} x_i^{(t+1)} = \\gamma^{(t)} \\left( x_i^{(t)}, \\sum_{j \\in \\mathcal{N}(i)} m^{(t)} \\left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \\right) \\right) \\end{align} $$ Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $m^{(t)}$ is the message function which is used to aggregate information from the neighbors of each node, $e_{ij}^{(t)}$ is the edge information between node $i$ and node $j$ at time $t$, and $\\gamma^{(t)}$ is the update function to update the hidden state of each node given the aggregated information from the neighbors of each node, usually it’s a neural network.\nIllustration of aggregating information from the neighbors of each node\nThe above figure shows the example when considering the node $x_i^{(t)}$ at time $t$, all the information from the neighbors of node $i$ (blue nodes on the left) is aggregated into a message $m^{(t)}$ such as the sum of the hidden states of the neighbors of node $i$ which can be written as:\n$$ \\begin{align} m^{(t)} \\left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \\right) = \\sum_{j \\in \\mathcal{N}(i)} x_j^{(t)} \\end{align} $$ Then, the message $m^{(t)}$ is used to update the hidden state of node $i$ at time $t+1$ with the update function $\\gamma^{(t)}$. To make less abstract, let’s consider the case when the update function $\\gamma^{(t)}$ is a neural network with one hidden layer. The following equation shows the update function $\\gamma^{(t)}$:\n$$ \\begin{align} x_i^{(t+1)} \u0026= \\gamma^{(t)} \\left( x_i^{(t)}, \\sum_{j \\in \\mathcal{N}(i)} x_j^{(t)} \\right) \\\\ x_i^{(t+1)} \u0026= \\sigma \\left( W_1 x_i^{(t)} + W_2 \\sum_{j \\in \\mathcal{N}(i)} x_j^{(t)} \\right) \\end{align} $$ This $\\sigma$ is any non-linear function. $W_1$ and $W_2$ are weight matrices.\nIn order to classify the nodes, we need to transform the hidden state of each node into an output. The transformation step introduce a another function $f$ to transform the hidden state of each node into an output. The following equation shows the transformation step:\n$$ \\begin{align} y_i = f \\left( W x_i^{(T)} \\right) \\end{align} $$ Where $y_i$ is the output of node $i$, $W$ is a weight matrix, and $f$ is a non-linear function such as ReLU or tanh.\nHere is how MPNN works in practice:\nInitialize the hidden state of each node.\nFor $t$ in $1, \\dots, T$:\nCompute the message $m^{(t)}$ for each node.\nUpdate the hidden state of each node.\nTransform the hidden state of each node into an output.\nGraph Convolutional Network The Graph Convolutional Network (GCN) is a type of GNN proposed by Kipf et al. in this paper. It is a simplified version of the MPNN. The following equation shows the GCN:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_i d_j}} x_j^{(t)} W^{(t)} \\end{align} $$ $d_i$ is the degree of node $i$. In other words, it is the number of neighbors of node $i$.\nGCN uses degree normalization to aggregate information from the neighbors of each node instead of summing the hidden states of the neighbors of each node. This helps to avoid the problem of the exploding gradient.\nGraph Convolutional Networks (GCNs) can be seen as a generalization of the convolutional neural networks (CNNs).\nGraph Attention Network The Graph Attention Network (GAT) is a type of GNN proposed by Veličković et al. in this paper. This enables each node to attend over its neighbors’ features which can be written as:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(t)} x_j^{(t)} W^{(t)} \\end{align} $$ $\\alpha_{ij}^{(t)}$ is the attention coefficient between node $i$ and node $j$ at time $t$. Simply put, the attention coefficient is a measure of how much attention should be paid to node $j$ when computing the hidden state of node $i$. The attention coefficient can be computed as follows:\n$$ \\begin{align} \\alpha_{ij}^{(t)} = \\frac{\\exp \\left(f \\left( x_i^{(t)}, x_j^{(t)} \\right) \\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp \\left(f \\left( x_i^{(t)}, x_k^{(t)} \\right) \\right)} \\end{align} $$ Where $f$ is a non-linear function such as ReLU or tanh.\nGraph Attention Networks (GATs) can be seen as a generalization of the attention mechanism used in the Transformer model.\nUp to this point, we have learned about the basics of graph terminology, the way to represent a graph, and the way to work with graph structure. Now, it’s time to apply what we have learned to solve practical problems. In the next section, we will use the GNN to solve a node classification problem.\nNode Classification in a Citation Network Dataset In this section, we will use the Cora dataset to demonstrate how to use the GNN to solve a node classification problem. The Cora dataset is a citation network dataset. It consists of 2708 scientific publications classified into one of seven classes:\nCase_Based Genetic_Algorithms Neural_Networks Probabilistic_Methods Reinforcement_Learning Rule_Learning Theory Let’s start by understanding the dataset. The following code shows how to load the Cora dataset:\nimport torch from torch_geometric.datasets import Planetoid dataset = Planetoid(root='data', name='Cora') print(dataset[0]) The output of the above code is shown below:\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) $x$ is a feature matrix of shape $N \\times F$, where $N$ is the number of nodes and $F$ is the number of features, in this case the node features represent the bag-of-words representation of the documents.\n$edge\\_index$ is an edge index matrix of shape $2 \\times E$, where $E$ is the number of edges. The edge index matrix stores the source and destination nodes of each edge.\n$y$ is a class vector of shape $N$, where $N$ is the number of nodes.\n$train\\_mask$ , $val\\_mask$ , and $test\\_mask$ are boolean vectors of shape $N$. They indicate whether the nodes belong to the training, validation, and test sets, respectively.\nThe following code shows how to visualize the Cora dataset:\nimport random import networkx as nx import matplotlib.pyplot as plt from torch_geometric.utils import to_networkx graph = dataset[0] G = to_networkx(graph) # sample 1000 nodes to visualize nodes = random.sample(G.nodes, 1000) G = G.subgraph(nodes) y = graph.y[nodes] plt.figure(figsize=(10, 7)) nx.draw(G, cmap='Set1', node_color=y, node_size=30, arrows=False) Network visualization of the Cora dataset\nNow, let’s build a GNN model to solve the node classification problem. The following code shows how to build a GNN model using the GCN:\nimport torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.nn import GCNConv class GNN(nn.Module): def __init__(self, in_channels, hidden_channels, out_channels): super().__init__() self.conv1 = GCNConv(in_channels, hidden_channels) self.conv2 = GCNConv(hidden_channels, out_channels) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = F.relu(x) x = F.dropout(x, training=self.training) x = self.conv2(x, edge_index) return x The following code shows how to train the GNN model:\nimport torch_geometric.transforms as T # split the dataset into training, validation, and test sets splitter = T.RandomNodeSplit(num_val=0.1, num_test=0.2) graph = splitter(dataset[0]) # build the model model = GNN(dataset.num_features, 16, dataset.num_classes) # train the model optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) def train(): model.train() optimizer.zero_grad() out = model(graph.x, graph.edge_index) loss = F.cross_entropy(out[graph.train_mask], graph.y[graph.train_mask]) loss.backward() optimizer.step() # evaluate the model @torch.no_grad() def test(): model.eval() out = model(graph.x, graph.edge_index) pred = out.argmax(dim=1) acc = pred[graph.test_mask] == graph.y[graph.test_mask] acc = int(acc.sum()) / int(graph.test_mask.sum()) return acc for epoch in range(1, 201): train() acc = test() print(f'Epoch: {epoch:03d}, Accuracy: {acc:.4f}') Here is a sample output of the above code:\nEpoch: 001, Accuracy: 0.3911 Epoch: 002, Accuracy: 0.4945 Epoch: 003, Accuracy: 0.5572 . . . Epoch: 198, Accuracy: 0.8875 Epoch: 199, Accuracy: 0.8856 Epoch: 200, Accuracy: 0.8838 That’s it! We have successfully built a GNN model to solve a node classification problem. Our model achieves an accuracy of 88.38% on the test set. What a great result!\nHere is a colab notebook that contains the code for this post.\nSummary Solving graph-related problems is not an easy task, it requires a lot of effort and time to process the graph data. Fortunately, the GNN offers a framework for solving graph-related problems in a simple and efficient way. So, if you are working on a graph-related problem, you should give the GNN a try.\nRefernces Graph Neural Networks: A Review of Methods and Applications Graph Convolutional Networks Graph Attention Networks PyTorch Geometric Intro to graph neural networks (ML Tech Talks) Graph Neural Networks with PyG on Node Classification, Link Prediction, and Anomaly Detection ",
  "wordCount" : "2099",
  "inLanguage": "en",
  "datePublished": "2023-08-13T00:00:00Z",
  "dateModified": "2023-08-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Nut Chukamphaeng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nutorbit.github.io/posts/graph_neural_network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "nutorbit",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nutorbit.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nutorbit.github.io/" accesskey="h" title="nutorbit (Alt + H)">nutorbit</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://nutorbit.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://nutorbit.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Graphs are everywhere
    </h1>
    <div class="post-meta"><span title='2023-08-13 00:00:00 +0000 UTC'>August 13, 2023</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Nut Chukamphaeng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-is-a-graph" aria-label="What is a graph?">What is a graph?</a></li>
                <li>
                    <a href="#what-is-a-graph-neural-network" aria-label="What is a Graph Neural Network?">What is a Graph Neural Network?</a><ul>
                        
                <li>
                    <a href="#message-passing-neural-network" aria-label="Message Passing Neural Network">Message Passing Neural Network</a></li>
                <li>
                    <a href="#graph-convolutional-network" aria-label="Graph Convolutional Network">Graph Convolutional Network</a></li>
                <li>
                    <a href="#graph-attention-network" aria-label="Graph Attention Network">Graph Attention Network</a></li></ul>
                </li>
                <li>
                    <a href="#node-classification-in-a-citation-network-dataset" aria-label="Node Classification in a Citation Network Dataset">Node Classification in a Citation Network Dataset</a></li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#refernces" aria-label="Refernces">Refernces</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.</p>
<p>Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge. For example, to classify the nodes in a social network, we need to extract features from the nodes and edges. Then, we need to design a model that can learn from the extracted features. Consequently, the model and data preprocessing pipeline are very specific to the task.</p>
<p>Nowdays, with the rise of deep learning, we can learn from graph structure without requiring a lot of domain knowledge. That&rsquo;s where Graph Neural Networks (GNNs) come in.</p>
<p>Graph Neural Network (GNN) is a neural network that can learn from graph structure. It provides a framework to learn from graph structure without requiring a lot of domain knowledge. Moreover, it can be applied to many tasks such as classifying nodes, predicting the connection between nodes, or predicting the graph structure.</p>
<p>To give you an idea of what GNN can do, here are some examples of applications:</p>
<ul>
<li>
<p><strong>Classifying nodes</strong>: predict the type of a person in a social network (student, professor, etc.) or predict the atom type in a molecule.</p>
</li>
<li>
<p><strong>Predicting the connection between nodes</strong>: predict the friendship between two people in a social network.</p>
</li>
<li>
<p><strong>Predicting the graph structure</strong>: predict the type of a molecule from its structure.</p>
</li>
</ul>
<p><img loading="lazy" src="images/tasks.svg" alt=""  />

<em>Examples of tasks that can be solved with GNNs</em></p>
<p>In this post, I will give you an overview of GNNs. Also, explaining in detail the different components of a GNN and how it works theoretically. This post will contain a lot of math. Please bear with me. Now, let&rsquo;s get started!</p>
<h2 id="what-is-a-graph">What is a graph?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-graph">#</a></h2>
<p>Graph is a data structure that consists of nodes and edges. The edges connect the nodes. Mathematically, a graph can be represented as a tuple $(V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. A graph can be directed or undirected. For simplicity, let&rsquo;s focus on undirected graphs.</p>
<p><img loading="lazy" src="images/graph.svg" alt=""  />

<em>Example of a graph with 4 nodes and 4 edges</em></p>
<p>In the above figure, the nodes are represented as circles ($V \in {1, 2, 3, 4}$) and the edges are represented as blue lines ($E \in {(1, 2), (1, 4), (2, 3), (2, 4)}$).</p>
<p>Another way to represent a graph is with an adjacency matrix. An adjacency matrix is a matrix $A$ of size $|V| \times |V|$ where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$ and $A_{ij} = 0$ otherwise. For example, the adjacency matrix of the above graph is:</p>

$$
A = 
\begin{bmatrix}
    0 & 1 & 0 & 1 \\
    1 & 0 & 1 & 1 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 \\
\end{bmatrix}
$$


<p>These two representations that I mentioned above are the most common and easy ways to represent a graph. Imagine if you want to add more information to the graph such as node information or edge information, this makes the graph representation more complicated, and it will be hard to work with.</p>
<h2 id="what-is-a-graph-neural-network">What is a Graph Neural Network?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-graph-neural-network">#</a></h2>
<p>Now that we know what a graph is, it&rsquo;s time to talk about the way to work with graph structure. I will assume that you are familiar with neural networks. If you are not, I recommend you to read <a href="https://towardsdatascience.com/understanding-neural-networks-19020b758230">this post</a> first.</p>
<p>A Graph Neural Network (GNN) is one type of neural network that can learn from graph structure to solve a task by aggregating information from the neighbors of each node. One of the well-known GNNs is the Message Passing Neural Network (MPNN) proposed by Gilmer et al. in <a href="https://arxiv.org/abs/1704.01212">this paper</a>.</p>
<p>To make this post easier to follow, let&rsquo;s focus on the task of classifying nodes.</p>
<h3 id="message-passing-neural-network">Message Passing Neural Network<a hidden class="anchor" aria-hidden="true" href="#message-passing-neural-network">#</a></h3>
<p>The Message Passing Neural Network (MPNN) consists of three steps: <strong>Initialization</strong>, <strong>Message Passing</strong>, and <strong>Transformation</strong>. The initialization step is used to initialize the hidden state of each node (also called node embedding). The message passing step is used to aggregate information from the neighbors of each node. The transformation step is used to transform the hidden state of each node to a new hidden state.</p>
<p>The following equation shows how to update the hidden state of each node at time $t+1$:</p>

$$

\begin{align}

    x_i^{(t+1)} = \gamma^{(t)} \left( x_i^{(t)}, \sum_{j \in \mathcal{N}(i)} m^{(t)} \left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \right) \right)

\end{align}

$$


<p>Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\mathcal{N}(i)$ is the set of neighbors of node $i$, $m^{(t)}$ is the message function which is used to aggregate information from the neighbors of each node, $e_{ij}^{(t)}$ is the edge information between node $i$ and node $j$ at time $t$, and $\gamma^{(t)}$ is the update function to update the hidden state of each node given the aggregated information from the neighbors of each node, usually it&rsquo;s a neural network.</p>
<p><img loading="lazy" src="images/gcn.svg" alt=""  />

<em>Illustration of aggregating information from the neighbors of each node</em></p>
<p>The above figure shows the example when considering the node $x_i^{(t)}$ at time $t$, all the information from the neighbors of node $i$ (blue nodes on the left) is aggregated into a message $m^{(t)}$ such as the sum of the hidden states of the neighbors of node $i$ which can be written as:</p>

$$

\begin{align}

    m^{(t)} \left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \right) = \sum_{j \in \mathcal{N}(i)} x_j^{(t)}

\end{align}

$$


<p>Then, the message $m^{(t)}$ is used to update the hidden state of node $i$ at time $t+1$ with the update function $\gamma^{(t)}$. To make less abstract, let&rsquo;s consider the case when the update function $\gamma^{(t)}$ is a neural network with one hidden layer. The following equation shows the update function $\gamma^{(t)}$:</p>

$$

\begin{align}

    x_i^{(t+1)} &= \gamma^{(t)} \left( x_i^{(t)}, \sum_{j \in \mathcal{N}(i)} x_j^{(t)} \right) \\
    x_i^{(t+1)} &= \sigma \left( W_1 x_i^{(t)} + W_2 \sum_{j \in \mathcal{N}(i)} x_j^{(t)} \right)

\end{align}

$$


<p>This $\sigma$ is any non-linear function. $W_1$ and $W_2$ are weight matrices.</p>
<p>In order to classify the nodes, we need to transform the hidden state of each node into an output. The transformation step introduce a another function $f$ to transform the hidden state of each node into an output. The following equation shows the transformation step:</p>

$$

\begin{align}

    y_i = f \left( W x_i^{(T)} \right)

\end{align}

$$


<p>Where $y_i$ is the output of node $i$, $W$ is a weight matrix, and $f$ is a non-linear function such as ReLU or tanh.</p>
<p>Here is how MPNN works in practice:</p>
<ol>
<li>
<p>Initialize the hidden state of each node.</p>
</li>
<li>
<p>For $t$ in $1, \dots, T$:</p>
<ol>
<li>
<p>Compute the message $m^{(t)}$ for each node.</p>
</li>
<li>
<p>Update the hidden state of each node.</p>
</li>
</ol>
</li>
<li>
<p>Transform the hidden state of each node into an output.</p>
</li>
</ol>
<h3 id="graph-convolutional-network">Graph Convolutional Network<a hidden class="anchor" aria-hidden="true" href="#graph-convolutional-network">#</a></h3>
<p>The Graph Convolutional Network (GCN) is a type of GNN proposed by Kipf et al. in <a href="https://arxiv.org/abs/1609.02907">this paper</a>. It is a simplified version of the MPNN. The following equation shows the GCN:</p>

$$

\begin{align}

    x_i^{(t+1)} = \sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{d_i d_j}} x_j^{(t)} W^{(t)}

\end{align}

$$


<p>$d_i$ is the degree of node $i$. In other words, it is the number of neighbors of node $i$.</p>
<p>GCN uses degree normalization to aggregate information from the neighbors of each node instead of summing the hidden states of the neighbors of each node. This helps to avoid the problem of the exploding gradient.</p>
<blockquote>
<p>Graph Convolutional Networks (GCNs) can be seen as a generalization of the convolutional neural networks (CNNs).</p>
</blockquote>
<h3 id="graph-attention-network">Graph Attention Network<a hidden class="anchor" aria-hidden="true" href="#graph-attention-network">#</a></h3>
<p>The Graph Attention Network (GAT) is a type of GNN proposed by Veličković et al. in <a href="https://arxiv.org/abs/1710.10903">this paper</a>. This enables each node to attend over its neighbors&rsquo; features which can be written as:</p>

$$

\begin{align}

    x_i^{(t+1)} = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(t)} x_j^{(t)} W^{(t)}

\end{align}

$$


<p>$\alpha_{ij}^{(t)}$ is the attention coefficient between node $i$ and node $j$ at time $t$. Simply put, the attention coefficient is a measure of how much attention should be paid to node $j$ when computing the hidden state of node $i$. The attention coefficient can be computed as follows:</p>

$$

\begin{align}

    \alpha_{ij}^{(t)} = \frac{\exp \left(f \left( x_i^{(t)}, x_j^{(t)} \right) \right)}{\sum_{k \in \mathcal{N}(i)} \exp \left(f \left( x_i^{(t)}, x_k^{(t)} \right) \right)}

\end{align}

$$


<p>Where $f$ is a non-linear function such as ReLU or tanh.</p>
<blockquote>
<p>Graph Attention Networks (GATs) can be seen as a generalization of the attention mechanism used in the Transformer model.</p>
</blockquote>
<hr>
<p>Up to this point, we have learned about the basics of graph terminology, the way to represent a graph, and the way to work with graph structure. Now, it&rsquo;s time to apply what we have learned to solve practical problems. In the next section, we will use the GNN to solve a node classification problem.</p>
<h2 id="node-classification-in-a-citation-network-dataset">Node Classification in a Citation Network Dataset<a hidden class="anchor" aria-hidden="true" href="#node-classification-in-a-citation-network-dataset">#</a></h2>
<p>In this section, we will use the Cora dataset to demonstrate how to use the GNN to solve a node classification problem. The Cora dataset is a citation network dataset. It consists of 2708 scientific publications classified into one of seven classes:</p>
<ul>
<li>Case_Based</li>
<li>Genetic_Algorithms</li>
<li>Neural_Networks</li>
<li>Probabilistic_Methods</li>
<li>Reinforcement_Learning</li>
<li>Rule_Learning</li>
<li>Theory</li>
</ul>
<p>Let&rsquo;s start by understanding the dataset. The following code shows how to load the Cora dataset:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cora&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></div><p>The output of the above code is shown below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Data</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">,</span> <span class="mi">1433</span><span class="p">],</span> <span class="n">edge_index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10556</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">train_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">val_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">test_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">])</span>
</span></span></code></pre></div><p>$x$ is a feature matrix of shape $N \times F$, where $N$ is the number of nodes and $F$ is the number of features, in this case the node features represent the bag-of-words representation of the documents.</p>
<p>$edge\_index$
 is an edge index matrix of shape $2 \times E$, where $E$ is the number of edges. The edge index matrix stores the source and destination nodes of each edge.</p>
<p>$y$ is a class vector of shape $N$, where $N$ is the number of nodes.</p>
<p>$train\_mask$
, $val\_mask$
, and $test\_mask$
 are boolean vectors of shape $N$. They indicate whether the nodes belong to the training, validation, and test sets, respectively.</p>
<p>The following code shows how to visualize the Cora dataset:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">to_networkx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">graph</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">G</span> <span class="o">=</span> <span class="n">to_networkx</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># sample 1000 nodes to visualize</span>
</span></span><span class="line"><span class="cl"><span class="n">nodes</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">subgraph</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">nodes</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Set1&#39;</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="images/cora.png" alt=""  />

<em>Network visualization of the Cora dataset</em></p>
<p>Now, let&rsquo;s build a GNN model to solve the node classification problem. The following code shows how to build a GNN model using the GCN:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div><p>The following code shows how to train the GNN model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="nn">T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># split the dataset into training, validation, and test sets</span>
</span></span><span class="line"><span class="cl"><span class="n">splitter</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">RandomNodeSplit</span><span class="p">(</span><span class="n">num_val</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_test</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">graph</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># build the model</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">GNN</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># train the model</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">train_mask</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate the model</span>
</span></span><span class="line"><span class="cl"><span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pred</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="p">]</span> <span class="o">==</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">acc</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">201</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>Here is a sample output of the above code:</p>
<pre tabindex="0"><code>Epoch: 001, Accuracy: 0.3911
Epoch: 002, Accuracy: 0.4945
Epoch: 003, Accuracy: 0.5572
.
.
.
Epoch: 198, Accuracy: 0.8875
Epoch: 199, Accuracy: 0.8856
Epoch: 200, Accuracy: 0.8838
</code></pre><p>That&rsquo;s it! We have successfully built a GNN model to solve a node classification problem. Our model achieves an accuracy of 88.38% on the test set. What a great result!</p>
<p>Here is a <a href="https://colab.research.google.com/drive/1rfzanyffBn3tt-7bOGMtffZpLh1vAkc4?usp=sharing">colab notebook</a> that contains the code for this post.</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>Solving graph-related problems is not an easy task, it requires a lot of effort and time to process the graph data. Fortunately, the GNN offers a framework for solving graph-related problems in a simple and efficient way. So, if you are working on a graph-related problem, you should give the GNN a try.</p>
<h2 id="refernces">Refernces<a hidden class="anchor" aria-hidden="true" href="#refernces">#</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1812.08434">Graph Neural Networks: A Review of Methods and Applications</a></li>
<li><a href="https://arxiv.org/abs/1609.02907">Graph Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></li>
<li><a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a></li>
<li><a href="https://youtu.be/8owQBFAHw7E">Intro to graph neural networks (ML Tech Talks)
</a></li>
<li><a href="https://towardsdatascience.com/graph-neural-networks-with-pyg-on-node-classification-link-prediction-and-anomaly-detection-14aa38fe1275">Graph Neural Networks with PyG on Node Classification, Link Prediction, and Anomaly Detection</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://nutorbit.github.io/tags/graph-neural-network/">graph neural network</a></li>
    </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on twitter"
        href="https://twitter.com/intent/tweet/?text=Graphs%20are%20everywhere&amp;url=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f&amp;hashtags=graphneuralnetwork">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f&amp;title=Graphs%20are%20everywhere&amp;summary=Graphs%20are%20everywhere&amp;source=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f&title=Graphs%20are%20everywhere">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on whatsapp"
        href="https://api.whatsapp.com/send?text=Graphs%20are%20everywhere%20-%20https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graphs are everywhere on telegram"
        href="https://telegram.me/share/url?text=Graphs%20are%20everywhere&amp;url=https%3a%2f%2fnutorbit.github.io%2fposts%2fgraph_neural_network%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://nutorbit.github.io/">nutorbit</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Graphs are everywhere | nutorbit</title>
<meta name="keywords" content="graph neural network">
<meta name="description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge.">
<meta name="author" content="Nut Chukamphaeng">
<link rel="canonical" href="https://nutorbit.github.io/posts/graph_neural_network/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://nutorbit.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nutorbit.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nutorbit.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nutorbit.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nutorbit.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
    integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"
    crossorigin="anonymous"
    referrerpolicy="no-referrer">

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
    integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "\\[", right: "\\]", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false},
      ],
    });
  });
</script>
<meta property="og:title" content="Graphs are everywhere" />
<meta property="og:description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nutorbit.github.io/posts/graph_neural_network/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-08-13T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Graphs are everywhere"/>
<meta name="twitter:description" content="If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.
Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nutorbit.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Graphs are everywhere",
      "item": "https://nutorbit.github.io/posts/graph_neural_network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Graphs are everywhere",
  "name": "Graphs are everywhere",
  "description": "If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge.",
  "keywords": [
    "graph neural network"
  ],
  "articleBody": "If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge. For example, to classify the nodes in a social network, we need to extract features from the nodes and edges. Then, we need to design a model that can learn from the extracted features. Consequently, the model and data preprocessing pipeline are very specific to the task.\nNowdays, with the rise of deep learning, we can learn from graph structure without requiring a lot of domain knowledge. That’s where Graph Neural Networks (GNNs) come in.\nGraph Neural Network (GNN) is a neural network that can learn from graph structure. It provides a framework to learn from graph structure without requiring a lot of domain knowledge. Moreover, it can be applied to many tasks such as classifying nodes, predicting the connection between nodes, or predicting the graph structure.\nTo give you an idea of what GNN can do, here are some examples of applications:\nClassifying nodes: predict the type of a person in a social network (student, professor, etc.) or predict the atom type in a molecule.\nPredicting the connection between nodes: predict the friendship between two people in a social network.\nPredicting the graph structure: predict the type of a molecule from its structure.\nExamples of tasks that can be solved with GNNs\nIn this post, I will give you an overview of GNNs. Also, explaining in detail the different components of a GNN and how it works theoretically. This post will contain a lot of math. Please bear with me. Now, let’s get started!\nWhat is a graph? Graph is a data structure that consists of nodes and edges. The edges connect the nodes. Mathematically, a graph can be represented as a tuple $(V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. A graph can be directed or undirected. For simplicity, let’s focus on undirected graphs.\nExample of a graph with 4 nodes and 4 edges\nIn the above figure, the nodes are represented as circles ($V = {1, 2, 3, 4}$) and the edges are represented as blue lines ($E = {(1, 2), (1, 4), (2, 3), (2, 4)}$).\nAnother way to represent a graph is with an adjacency matrix. An adjacency matrix is a matrix $A$ of size $|V| \\times |V|$ where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$ and $A_{ij} = 0$ otherwise. For example, the adjacency matrix of the above graph is:\n$$ A = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 1 \u0026 1 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 0 \u0026 0 \\\\ \\end{bmatrix} $$ What is a Graph Neural Network? Now that we know what a graph is, it’s time to talk about the way to work with graph structure. I will assume that you are familiar with neural networks. If you are not, I recommend you to read this post first.\nA Graph Neural Network (GNN) is one type of neural network that can learn from graph structure to solve a task by aggregating information from the neighbors of each node. One of the well-known GNNs is the Message Passing Neural Network (MPNN) proposed by Gilmer et al. in this paper.\nTo make this post easier to follow, let’s focus on the task of classifying nodes.\nMessage Passing Neural Network The Message Passing Neural Network (MPNN) consists of three steps: Initialization, Message Passing, and Transformation. The initialization step is used to initialize the hidden state of each node. The message passing step is used to aggregate information from the neighbors of each node. The transformation step is used to transform the hidden state of each node into an output. The following figure shows the three steps of the MPNN:\n$$ \\begin{align} x_i^{(t+1)} = \\gamma^{(t)} \\left( x_i^{(t)}, \\sum_{j \\in \\mathcal{N}(i)} m^{(t)} \\left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \\right) \\right) \\end{align} $$ Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $m^{(t)}$ is the message function, $\\gamma^{(t)}$ is the update function, and $e_{ij}^{(t)}$ is the edge attribute between node $i$ and node $j$ at time $t$. The message function $m^{(t)}$ is used to aggregate information from the neighbors of each node. The update function $\\gamma^{(t)}$ is used to update the hidden state of each node.\nIllustration of aggregating information from the neighbors of each node\nThe above figure shows when considering the node $x_i^{(t)}$ at time $t$, all the information from the neighbors of node $i$ (blue nodes on the left) is aggregated into a message $m^{(t)}$. Then, the message $m^{(t)}$ is used to update the hidden state of node $i$ to $x_i^{(t+1)}$ (blue node on the right).\nIn order to classify the nodes, we need to transform the hidden state of each node into an output. The transformation step is done by applying a non-linear function to the hidden state of each node. The following figure shows the transformation step:\n$$ \\begin{align} y_i = f \\left( W x_i^{(T)} \\right) \\end{align} $$ Where $y_i$ is the output of node $i$, $W$ is a weight matrix, and $f$ is a non-linear function such as ReLU or tanh.\nHere is how MPNN works in practice:\nInitialize the hidden state of each node.\nFor $t$ in $1, \\dots, T$:\nCompute the message $m^{(t)}$ for each node.\nUpdate the hidden state of each node.\nTransform the hidden state of each node into an output.\nGraph Convolutional Network The Graph Convolutional Network (GCN) is a type of GNN proposed by Kipf et al. in this paper. It is a simplified version of the MPNN. The following figure shows the GCN:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_i d_j}} x_j^{(t)} W^{(t)} \\end{align} $$ Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $d_i$ is the degree of node $i$, $W^{(t)}$ is a weight matrix, and $x_j^{(t)}$ is the hidden state of node $j$ at time $t$.\nGraph Convolutional Networks (GCNs) can be seen as a generalization of the convolutional neural networks (CNNs).\nGraph Attention Network The Graph Attention Network (GAT) is a type of GNN proposed by Veličković et al. in this paper. It is an extension of the GCN. The following figure shows the GAT:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(t)} x_j^{(t)} W^{(t)} \\end{align} $$ $\\alpha_{ij}^{(t)}$ is the attention coefficient between node $i$ and node $j$ at time $t$. Simply put, the attention coefficient is a measure of how much attention should be paid to node $j$ when computing the hidden state of node $i$. The attention coefficient can be computed as follows:\n$$ \\begin{align} \\alpha_{ij}^{(t)} = \\frac{\\exp \\left(f \\left( x_i^{(t)}, x_j^{(t)} \\right) \\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp \\left(f \\left( x_i^{(t)}, x_k^{(t)} \\right) \\right)} \\end{align} $$ Where $f$ is a non-linear function such as ReLU or tanh.\nGraph Attention Networks (GATs) can be seen as a generalization of the attention mechanism used in the Transformer model.\nUp to this point, we have learned about the basics of graph terminology, the way to represent a graph, and the way to work with graph structure. Now, it’s time to apply what we have learned to solve practical problems. In the next section, we will use the GNN to solve a node classification problem.\nNode Classification in a Citation Network Dataset In this section, we will use the Cora dataset to demonstrate how to use the GNN to solve a node classification problem. The Cora dataset is a citation network dataset. It consists of 2708 scientific publications classified into one of seven classes:\nCase_Based Genetic_Algorithms Neural_Networks Probabilistic_Methods Reinforcement_Learning Rule_Learning Theory Let’s start by understanding the dataset. The following code shows how to load the Cora dataset:\nimport torch from torch_geometric.datasets import Planetoid dataset = Planetoid(root='data', name='Cora') print(dataset[0]) The output of the above code is shown below:\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) $x$ is a feature matrix of shape $N \\times F$, where $N$ is the number of nodes and $F$ is the number of features, in this case the node features represent the bag-of-words representation of the documents.\n$edge\\_index$ is an edge index matrix of shape $2 \\times E$, where $E$ is the number of edges. The edge index matrix stores the source and destination nodes of each edge.\n$y$ is a class vector of shape $N$, where $N$ is the number of nodes.\n$train\\_mask$ , $val\\_mask$ , and $test\\_mask$ are boolean vectors of shape $N$. They indicate whether the nodes belong to the training, validation, and test sets, respectively.\nThe following code shows how to visualize the Cora dataset:\nimport random import networkx as nx import matplotlib.pyplot as plt from torch_geometric.utils import to_networkx graph = dataset[0] G = to_networkx(graph) # sample 1000 nodes to visualize nodes = random.sample(G.nodes, 1000) G = G.subgraph(nodes) y = graph.y[nodes] plt.figure(figsize=(10, 7)) nx.draw(G, cmap='Set1', node_color=y, node_size=30, arrows=False) Network visualization of the Cora dataset\nNow, let’s build a GNN model to solve the node classification problem. The following code shows how to build a GNN model using the GCN:\nimport torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.nn import GCNConv class GNN(nn.Module): def __init__(self, in_channels, hidden_channels, out_channels): super().__init__() self.conv1 = GCNConv(in_channels, hidden_channels) self.conv2 = GCNConv(hidden_channels, out_channels) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = F.relu(x) x = F.dropout(x, training=self.training) x = self.conv2(x, edge_index) return x The following code shows how to train the GNN model:\nimport torch_geometric.transforms as T # split the dataset into training, validation, and test sets splitter = T.RandomNodeSplit(num_val=0.1, num_test=0.2) graph = splitter(dataset[0]) # build the model model = GNN(dataset.num_features, 16, dataset.num_classes) # train the model optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) def train(): model.train() optimizer.zero_grad() out = model(graph.x, graph.edge_index) loss = F.cross_entropy(out[graph.train_mask], graph.y[graph.train_mask]) loss.backward() optimizer.step() # evaluate the model @torch.no_grad() def test(): model.eval() out = model(graph.x, graph.edge_index) pred = out.argmax(dim=1) acc = pred[graph.test_mask] == graph.y[graph.test_mask] acc = int(acc.sum()) / int(graph.test_mask.sum()) return acc for epoch in range(1, 201): train() acc = test() print(f'Epoch: {epoch:03d}, Accuracy: {acc:.4f}') Here is a sample output of the above code:\nEpoch: 001, Accuracy: 0.3911 Epoch: 002, Accuracy: 0.4945 Epoch: 003, Accuracy: 0.5572 . . . Epoch: 198, Accuracy: 0.8875 Epoch: 199, Accuracy: 0.8856 Epoch: 200, Accuracy: 0.8838 That’s it! We have successfully built a GNN model to solve a node classification problem. Our model achieves an accuracy of 88.38% on the test set. What a great result!\nHere is a colab notebook that contains the code for this post.\nSummary As you can see, the GNN is a powerful tool for solving graph-related problems. It can be used to solve a wide range of problems such as node classification, link prediction, and graph classification.\nRefernces Graph Neural Networks: A Review of Methods and Applications Graph Convolutional Networks Graph Attention Networks PyTorch Geometric Intro to graph neural networks (ML Tech Talks) Graph Neural Networks with PyG on Node Classification, Link Prediction, and Anomaly Detection ",
  "wordCount" : "1892",
  "inLanguage": "en",
  "datePublished": "2023-08-13T00:00:00Z",
  "dateModified": "2023-08-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Nut Chukamphaeng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nutorbit.github.io/posts/graph_neural_network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "nutorbit",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nutorbit.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nutorbit.github.io/" accesskey="h" title="nutorbit (Alt + H)">nutorbit</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://nutorbit.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://nutorbit.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Graphs are everywhere
    </h1>
    <div class="post-meta"><span title='2023-08-13 00:00:00 +0000 UTC'>August 13, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Nut Chukamphaeng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-is-a-graph" aria-label="What is a graph?">What is a graph?</a></li>
                <li>
                    <a href="#what-is-a-graph-neural-network" aria-label="What is a Graph Neural Network?">What is a Graph Neural Network?</a><ul>
                        
                <li>
                    <a href="#message-passing-neural-network" aria-label="Message Passing Neural Network">Message Passing Neural Network</a></li>
                <li>
                    <a href="#graph-convolutional-network" aria-label="Graph Convolutional Network">Graph Convolutional Network</a></li>
                <li>
                    <a href="#graph-attention-network" aria-label="Graph Attention Network">Graph Attention Network</a></li></ul>
                </li>
                <li>
                    <a href="#node-classification-in-a-citation-network-dataset" aria-label="Node Classification in a Citation Network Dataset">Node Classification in a Citation Network Dataset</a></li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#refernces" aria-label="Refernces">Refernces</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.</p>
<p>Tackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge. For example, to classify the nodes in a social network, we need to extract features from the nodes and edges. Then, we need to design a model that can learn from the extracted features. Consequently, the model and data preprocessing pipeline are very specific to the task.</p>
<p>Nowdays, with the rise of deep learning, we can learn from graph structure without requiring a lot of domain knowledge. That&rsquo;s where Graph Neural Networks (GNNs) come in.</p>
<p>Graph Neural Network (GNN) is a neural network that can learn from graph structure. It provides a framework to learn from graph structure without requiring a lot of domain knowledge. Moreover, it can be applied to many tasks such as classifying nodes, predicting the connection between nodes, or predicting the graph structure.</p>
<p>To give you an idea of what GNN can do, here are some examples of applications:</p>
<ul>
<li>
<p><strong>Classifying nodes</strong>: predict the type of a person in a social network (student, professor, etc.) or predict the atom type in a molecule.</p>
</li>
<li>
<p><strong>Predicting the connection between nodes</strong>: predict the friendship between two people in a social network.</p>
</li>
<li>
<p><strong>Predicting the graph structure</strong>: predict the type of a molecule from its structure.</p>
</li>
</ul>
<p><img loading="lazy" src="images/tasks.svg" alt=""  />

<em>Examples of tasks that can be solved with GNNs</em></p>
<p>In this post, I will give you an overview of GNNs. Also, explaining in detail the different components of a GNN and how it works theoretically. This post will contain a lot of math. Please bear with me. Now, let&rsquo;s get started!</p>
<h2 id="what-is-a-graph">What is a graph?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-graph">#</a></h2>
<p>Graph is a data structure that consists of nodes and edges. The edges connect the nodes. Mathematically, a graph can be represented as a tuple $(V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. A graph can be directed or undirected. For simplicity, let&rsquo;s focus on undirected graphs.</p>
<p><img loading="lazy" src="images/graph.svg" alt=""  />

<em>Example of a graph with 4 nodes and 4 edges</em></p>
<p>In the above figure, the nodes are represented as circles ($V = {1, 2, 3, 4}$) and the edges are represented as blue lines ($E = {(1, 2), (1, 4), (2, 3), (2, 4)}$).</p>
<p>Another way to represent a graph is with an adjacency matrix. An adjacency matrix is a matrix $A$ of size $|V| \times |V|$ where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$ and $A_{ij} = 0$ otherwise. For example, the adjacency matrix of the above graph is:</p>

$$
A = 
\begin{bmatrix}
    0 & 1 & 0 & 1 \\
    1 & 0 & 1 & 1 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 \\
\end{bmatrix}
$$


<h2 id="what-is-a-graph-neural-network">What is a Graph Neural Network?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-graph-neural-network">#</a></h2>
<p>Now that we know what a graph is, it&rsquo;s time to talk about the way to work with graph structure. I will assume that you are familiar with neural networks. If you are not, I recommend you to read <a href="https://towardsdatascience.com/understanding-neural-networks-19020b758230">this post</a> first.</p>
<p>A Graph Neural Network (GNN) is one type of neural network that can learn from graph structure to solve a task by aggregating information from the neighbors of each node. One of the well-known GNNs is the Message Passing Neural Network (MPNN) proposed by Gilmer et al. in <a href="https://arxiv.org/abs/1704.01212">this paper</a>.</p>
<p>To make this post easier to follow, let&rsquo;s focus on the task of classifying nodes.</p>
<h3 id="message-passing-neural-network">Message Passing Neural Network<a hidden class="anchor" aria-hidden="true" href="#message-passing-neural-network">#</a></h3>
<p>The Message Passing Neural Network (MPNN) consists of three steps: <strong>Initialization</strong>, <strong>Message Passing</strong>, and <strong>Transformation</strong>. The initialization step is used to initialize the hidden state of each node. The message passing step is used to aggregate information from the neighbors of each node. The transformation step is used to transform the hidden state of each node into an output. The following figure shows the three steps of the MPNN:</p>

$$

\begin{align}

    x_i^{(t+1)} = \gamma^{(t)} \left( x_i^{(t)}, \sum_{j \in \mathcal{N}(i)} m^{(t)} \left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \right) \right)

\end{align}

$$


<p>Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\mathcal{N}(i)$ is the set of neighbors of node $i$, $m^{(t)}$ is the message function, $\gamma^{(t)}$ is the update function, and $e_{ij}^{(t)}$ is the edge attribute between node $i$ and node $j$ at time $t$. The message function $m^{(t)}$ is used to aggregate information from the neighbors of each node. The update function $\gamma^{(t)}$ is used to update the hidden state of each node.</p>
<p><img loading="lazy" src="images/gcn.svg" alt=""  />

<em>Illustration of aggregating information from the neighbors of each node</em></p>
<p>The above figure shows when considering the node $x_i^{(t)}$ at time $t$, all the information from the neighbors of node $i$ (blue nodes on the left) is aggregated into a message $m^{(t)}$. Then, the message $m^{(t)}$ is used to update the hidden state of node $i$ to $x_i^{(t+1)}$ (blue node on the right).</p>
<p>In order to classify the nodes, we need to transform the hidden state of each node into an output. The transformation step is done by applying a non-linear function to the hidden state of each node. The following figure shows the transformation step:</p>

$$

\begin{align}

    y_i = f \left( W x_i^{(T)} \right)

\end{align}

$$


<p>Where $y_i$ is the output of node $i$, $W$ is a weight matrix, and $f$ is a non-linear function such as ReLU or tanh.</p>
<p>Here is how MPNN works in practice:</p>
<ol>
<li>
<p>Initialize the hidden state of each node.</p>
</li>
<li>
<p>For $t$ in $1, \dots, T$:</p>
<ol>
<li>
<p>Compute the message $m^{(t)}$ for each node.</p>
</li>
<li>
<p>Update the hidden state of each node.</p>
</li>
</ol>
</li>
<li>
<p>Transform the hidden state of each node into an output.</p>
</li>
</ol>
<h3 id="graph-convolutional-network">Graph Convolutional Network<a hidden class="anchor" aria-hidden="true" href="#graph-convolutional-network">#</a></h3>
<p>The Graph Convolutional Network (GCN) is a type of GNN proposed by Kipf et al. in <a href="https://arxiv.org/abs/1609.02907">this paper</a>. It is a simplified version of the MPNN. The following figure shows the GCN:</p>

$$

\begin{align}

    x_i^{(t+1)} = \sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{d_i d_j}} x_j^{(t)} W^{(t)}

\end{align}

$$


<p>Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\mathcal{N}(i)$ is the set of neighbors of node $i$, $d_i$ is the degree of node $i$, $W^{(t)}$ is a weight matrix, and $x_j^{(t)}$ is the hidden state of node $j$ at time $t$.</p>
<blockquote>
<p>Graph Convolutional Networks (GCNs) can be seen as a generalization of the convolutional neural networks (CNNs).</p>
</blockquote>
<h3 id="graph-attention-network">Graph Attention Network<a hidden class="anchor" aria-hidden="true" href="#graph-attention-network">#</a></h3>
<p>The Graph Attention Network (GAT) is a type of GNN proposed by Veličković et al. in <a href="https://arxiv.org/abs/1710.10903">this paper</a>. It is an extension of the GCN. The following figure shows the GAT:</p>

$$

\begin{align}

    x_i^{(t+1)} = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(t)} x_j^{(t)} W^{(t)}

\end{align}

$$


<p>$\alpha_{ij}^{(t)}$ is the attention coefficient between node $i$ and node $j$ at time $t$. Simply put, the attention coefficient is a measure of how much attention should be paid to node $j$ when computing the hidden state of node $i$. The attention coefficient can be computed as follows:</p>

$$

\begin{align}

    \alpha_{ij}^{(t)} = \frac{\exp \left(f \left( x_i^{(t)}, x_j^{(t)} \right) \right)}{\sum_{k \in \mathcal{N}(i)} \exp \left(f \left( x_i^{(t)}, x_k^{(t)} \right) \right)}

\end{align}

$$


<p>Where $f$ is a non-linear function such as ReLU or tanh.</p>
<blockquote>
<p>Graph Attention Networks (GATs) can be seen as a generalization of the attention mechanism used in the Transformer model.</p>
</blockquote>
<hr>
<p>Up to this point, we have learned about the basics of graph terminology, the way to represent a graph, and the way to work with graph structure. Now, it&rsquo;s time to apply what we have learned to solve practical problems. In the next section, we will use the GNN to solve a node classification problem.</p>
<h2 id="node-classification-in-a-citation-network-dataset">Node Classification in a Citation Network Dataset<a hidden class="anchor" aria-hidden="true" href="#node-classification-in-a-citation-network-dataset">#</a></h2>
<p>In this section, we will use the Cora dataset to demonstrate how to use the GNN to solve a node classification problem. The Cora dataset is a citation network dataset. It consists of 2708 scientific publications classified into one of seven classes:</p>
<ul>
<li>Case_Based</li>
<li>Genetic_Algorithms</li>
<li>Neural_Networks</li>
<li>Probabilistic_Methods</li>
<li>Reinforcement_Learning</li>
<li>Rule_Learning</li>
<li>Theory</li>
</ul>
<p>Let&rsquo;s start by understanding the dataset. The following code shows how to load the Cora dataset:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cora&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></div><p>The output of the above code is shown below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Data</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">,</span> <span class="mi">1433</span><span class="p">],</span> <span class="n">edge_index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10556</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">train_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">val_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">],</span> <span class="n">test_mask</span><span class="o">=</span><span class="p">[</span><span class="mi">2708</span><span class="p">])</span>
</span></span></code></pre></div><p>$x$ is a feature matrix of shape $N \times F$, where $N$ is the number of nodes and $F$ is the number of features, in this case the node features represent the bag-of-words representation of the documents.</p>
<p>$edge\_index$
 is an edge index matrix of shape $2 \times E$, where $E$ is the number of edges. The edge index matrix stores the source and destination nodes of each edge.</p>
<p>$y$ is a class vector of shape $N$, where $N$ is the number of nodes.</p>
<p>$train\_mask$
, $val\_mask$
, and $test\_mask$
 are boolean vectors of shape $N$. They indicate whether the nodes belong to the training, validation, and test sets, respectively.</p>
<p>The following code shows how to visualize the Cora dataset:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">to_networkx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">graph</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">G</span> <span class="o">=</span> <span class="n">to_networkx</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># sample 1000 nodes to visualize</span>
</span></span><span class="line"><span class="cl"><span class="n">nodes</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">subgraph</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">nodes</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Set1&#39;</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="images/cora.png" alt=""  />

<em>Network visualization of the Cora dataset</em></p>
<p>Now, let&rsquo;s build a GNN model to solve the node classification problem. The following code shows how to build a GNN model using the GCN:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GCNConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div><p>The following code shows how to train the GNN model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="nn">T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># split the dataset into training, validation, and test sets</span>
</span></span><span class="line"><span class="cl"><span class="n">splitter</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">RandomNodeSplit</span><span class="p">(</span><span class="n">num_val</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_test</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">graph</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># build the model</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">GNN</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># train the model</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">train_mask</span><span class="p">],</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">train_mask</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate the model</span>
</span></span><span class="line"><span class="cl"><span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pred</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="p">]</span> <span class="o">==</span> <span class="n">graph</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">test_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">acc</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">201</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>Here is a sample output of the above code:</p>
<pre tabindex="0"><code>Epoch: 001, Accuracy: 0.3911
Epoch: 002, Accuracy: 0.4945
Epoch: 003, Accuracy: 0.5572
.
.
.
Epoch: 198, Accuracy: 0.8875
Epoch: 199, Accuracy: 0.8856
Epoch: 200, Accuracy: 0.8838
</code></pre><p>That&rsquo;s it! We have successfully built a GNN model to solve a node classification problem. Our model achieves an accuracy of 88.38% on the test set. What a great result!</p>
<p>Here is a <a href="https://colab.research.google.com/drive/1rfzanyffBn3tt-7bOGMtffZpLh1vAkc4?usp=sharing">colab notebook</a> that contains the code for this post.</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>As you can see, the GNN is a powerful tool for solving graph-related problems. It can be used to solve a wide range of problems such as node classification, link prediction, and graph classification.</p>
<h2 id="refernces">Refernces<a hidden class="anchor" aria-hidden="true" href="#refernces">#</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1812.08434">Graph Neural Networks: A Review of Methods and Applications</a></li>
<li><a href="https://arxiv.org/abs/1609.02907">Graph Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></li>
<li><a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a></li>
<li><a href="https://youtu.be/8owQBFAHw7E">Intro to graph neural networks (ML Tech Talks)
</a></li>
<li><a href="https://towardsdatascience.com/graph-neural-networks-with-pyg-on-node-classification-link-prediction-and-anomaly-detection-14aa38fe1275">Graph Neural Networks with PyG on Node Classification, Link Prediction, and Anomaly Detection</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://nutorbit.github.io/tags/graph-neural-network/">graph neural network</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://nutorbit.github.io/">nutorbit</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

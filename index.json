[{"content":"If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge. For example, to classify the nodes in a social network, we need to extract features from the nodes and edges. Then, we need to design a model that can learn from the extracted features. Consequently, the model and data preprocessing pipeline are very specific to the task.\nNowdays, with the rise of deep learning, we can learn from graph structure without requiring a lot of domain knowledge. That\u0026rsquo;s where Graph Neural Networks (GNNs) come in.\nGraph Neural Network (GNN) is a neural network that can learn from graph structure. It provides a framework to learn from graph structure without requiring a lot of domain knowledge. Moreover, it can be applied to many tasks such as classifying nodes, predicting the connection between nodes, or predicting the graph structure.\nTo give you an idea of what GNN can do, here are some examples of applications:\nClassifying nodes: predict the type of a person in a social network (student, professor, etc.) or predict the atom type in a molecule.\nPredicting the connection between nodes: predict the friendship between two people in a social network.\nPredicting the graph structure: predict the type of a molecule from its structure.\nExamples of tasks that can be solved with GNNs\nIn this post, I will give you an overview of GNNs. Also, explaining in detail the different components of a GNN and how it works theoretically. This post will contain a lot of math. Please bear with me. Now, let\u0026rsquo;s get started!\nWhat is a graph? Graph is a data structure that consists of nodes and edges. The edges connect the nodes. Mathematically, a graph can be represented as a tuple $(V, E)$ where $V$ is the set of nodes and $E$ is the set of edges. A graph can be directed or undirected. For simplicity, let\u0026rsquo;s focus on undirected graphs.\nExample of a graph with 4 nodes and 4 edges\nIn the above figure, the nodes are represented as circles ($V \\in {1, 2, 3, 4}$) and the edges are represented as blue lines ($E \\in {(1, 2), (1, 4), (2, 3), (2, 4)}$).\nAnother way to represent a graph is with an adjacency matrix. An adjacency matrix is a matrix $A$ of size $|V| \\times |V|$ where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$ and $A_{ij} = 0$ otherwise. For example, the adjacency matrix of the above graph is:\n$$ A = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 1 \u0026 1 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 0 \u0026 0 \\\\ \\end{bmatrix} $$ Understanding how to represent a graph is important as it will help you to understand how to work with graph structure.\nWhat is a Graph Neural Network? Now that we know what a graph is, it\u0026rsquo;s time to talk about the way to work with graph structure. I will assume that you are familiar with neural networks. If you are not, I recommend you to read this post first.\nA Graph Neural Network (GNN) is one type of neural network that can learn from graph structure to solve a task by aggregating information from the neighbors of each node. One of the well-known GNNs is the Message Passing Neural Network (MPNN) proposed by Gilmer et al. in this paper.\nTo make this post easier to follow, let\u0026rsquo;s focus on the task of classifying nodes.\nMessage Passing Neural Network The Message Passing Neural Network (MPNN) consists of three steps: Initialization, Message Passing, and Transformation. The initialization step is used to initialize the hidden state of each node. The message passing step is used to aggregate information from the neighbors of each node. The transformation step is used to transform the hidden state of each node into an output. The following equation shows the three steps of the MPNN:\n$$ \\begin{align} x_i^{(t+1)} = \\gamma^{(t)} \\left( x_i^{(t)}, \\sum_{j \\in \\mathcal{N}(i)} m^{(t)} \\left( x_i^{(t)}, x_j^{(t)}, e_{ij}^{(t)} \\right) \\right) \\end{align} $$ Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $m^{(t)}$ is the message function, $\\gamma^{(t)}$ is the update function, and $e_{ij}^{(t)}$ is the edge attribute between node $i$ and node $j$ at time $t$. The message function $m^{(t)}$ is used to aggregate information from the neighbors of each node. The update function $\\gamma^{(t)}$ is used to update the hidden state of each node.\nIllustration of aggregating information from the neighbors of each node\nThe above figure shows when considering the node $x_i^{(t)}$ at time $t$, all the information from the neighbors of node $i$ (blue nodes on the left) is aggregated into a message $m^{(t)}$. Then, the message $m^{(t)}$ is used to update the hidden state of node $i$ to $x_i^{(t+1)}$ (blue node on the right).\nIn order to classify the nodes, we need to transform the hidden state of each node into an output. The transformation step is done by applying a non-linear function to the hidden state of each node. The following equation shows the transformation step:\n$$ \\begin{align} y_i = f \\left( W x_i^{(T)} \\right) \\end{align} $$ Where $y_i$ is the output of node $i$, $W$ is a weight matrix, and $f$ is a non-linear function such as ReLU or tanh.\nHere is how MPNN works in practice:\nInitialize the hidden state of each node.\nFor $t$ in $1, \\dots, T$:\nCompute the message $m^{(t)}$ for each node.\nUpdate the hidden state of each node.\nTransform the hidden state of each node into an output.\nGraph Convolutional Network The Graph Convolutional Network (GCN) is a type of GNN proposed by Kipf et al. in this paper. It is a simplified version of the MPNN. The following equation shows the GCN:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_i d_j}} x_j^{(t)} W^{(t)} \\end{align} $$ Where $x_i^{(t)}$ is the hidden state of node $i$ at time $t$, $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $d_i$ is the degree of node $i$, $W^{(t)}$ is a weight matrix, and $x_j^{(t)}$ is the hidden state of node $j$ at time $t$.\nGraph Convolutional Networks (GCNs) can be seen as a generalization of the convolutional neural networks (CNNs).\nGraph Attention Network The Graph Attention Network (GAT) is a type of GNN proposed by Veličković et al. in this paper. It is an extension of the GCN. The following equation shows the GAT:\n$$ \\begin{align} x_i^{(t+1)} = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(t)} x_j^{(t)} W^{(t)} \\end{align} $$ $\\alpha_{ij}^{(t)}$ is the attention coefficient between node $i$ and node $j$ at time $t$. Simply put, the attention coefficient is a measure of how much attention should be paid to node $j$ when computing the hidden state of node $i$. The attention coefficient can be computed as follows:\n$$ \\begin{align} \\alpha_{ij}^{(t)} = \\frac{\\exp \\left(f \\left( x_i^{(t)}, x_j^{(t)} \\right) \\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp \\left(f \\left( x_i^{(t)}, x_k^{(t)} \\right) \\right)} \\end{align} $$ Where $f$ is a non-linear function such as ReLU or tanh.\nGraph Attention Networks (GATs) can be seen as a generalization of the attention mechanism used in the Transformer model.\nUp to this point, we have learned about the basics of graph terminology, the way to represent a graph, and the way to work with graph structure. Now, it\u0026rsquo;s time to apply what we have learned to solve practical problems. In the next section, we will use the GNN to solve a node classification problem.\nNode Classification in a Citation Network Dataset In this section, we will use the Cora dataset to demonstrate how to use the GNN to solve a node classification problem. The Cora dataset is a citation network dataset. It consists of 2708 scientific publications classified into one of seven classes:\nCase_Based Genetic_Algorithms Neural_Networks Probabilistic_Methods Reinforcement_Learning Rule_Learning Theory Let\u0026rsquo;s start by understanding the dataset. The following code shows how to load the Cora dataset:\nimport torch from torch_geometric.datasets import Planetoid dataset = Planetoid(root=\u0026#39;data\u0026#39;, name=\u0026#39;Cora\u0026#39;) print(dataset[0]) The output of the above code is shown below:\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) $x$ is a feature matrix of shape $N \\times F$, where $N$ is the number of nodes and $F$ is the number of features, in this case the node features represent the bag-of-words representation of the documents.\n$edge\\_index$ is an edge index matrix of shape $2 \\times E$, where $E$ is the number of edges. The edge index matrix stores the source and destination nodes of each edge.\n$y$ is a class vector of shape $N$, where $N$ is the number of nodes.\n$train\\_mask$ , $val\\_mask$ , and $test\\_mask$ are boolean vectors of shape $N$. They indicate whether the nodes belong to the training, validation, and test sets, respectively.\nThe following code shows how to visualize the Cora dataset:\nimport random import networkx as nx import matplotlib.pyplot as plt from torch_geometric.utils import to_networkx graph = dataset[0] G = to_networkx(graph) # sample 1000 nodes to visualize nodes = random.sample(G.nodes, 1000) G = G.subgraph(nodes) y = graph.y[nodes] plt.figure(figsize=(10, 7)) nx.draw(G, cmap=\u0026#39;Set1\u0026#39;, node_color=y, node_size=30, arrows=False) Network visualization of the Cora dataset\nNow, let\u0026rsquo;s build a GNN model to solve the node classification problem. The following code shows how to build a GNN model using the GCN:\nimport torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.nn import GCNConv class GNN(nn.Module): def __init__(self, in_channels, hidden_channels, out_channels): super().__init__() self.conv1 = GCNConv(in_channels, hidden_channels) self.conv2 = GCNConv(hidden_channels, out_channels) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = F.relu(x) x = F.dropout(x, training=self.training) x = self.conv2(x, edge_index) return x The following code shows how to train the GNN model:\nimport torch_geometric.transforms as T # split the dataset into training, validation, and test sets splitter = T.RandomNodeSplit(num_val=0.1, num_test=0.2) graph = splitter(dataset[0]) # build the model model = GNN(dataset.num_features, 16, dataset.num_classes) # train the model optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) def train(): model.train() optimizer.zero_grad() out = model(graph.x, graph.edge_index) loss = F.cross_entropy(out[graph.train_mask], graph.y[graph.train_mask]) loss.backward() optimizer.step() # evaluate the model @torch.no_grad() def test(): model.eval() out = model(graph.x, graph.edge_index) pred = out.argmax(dim=1) acc = pred[graph.test_mask] == graph.y[graph.test_mask] acc = int(acc.sum()) / int(graph.test_mask.sum()) return acc for epoch in range(1, 201): train() acc = test() print(f\u0026#39;Epoch: {epoch:03d}, Accuracy: {acc:.4f}\u0026#39;) Here is a sample output of the above code:\nEpoch: 001, Accuracy: 0.3911 Epoch: 002, Accuracy: 0.4945 Epoch: 003, Accuracy: 0.5572 . . . Epoch: 198, Accuracy: 0.8875 Epoch: 199, Accuracy: 0.8856 Epoch: 200, Accuracy: 0.8838 That\u0026rsquo;s it! We have successfully built a GNN model to solve a node classification problem. Our model achieves an accuracy of 88.38% on the test set. What a great result!\nHere is a colab notebook that contains the code for this post.\nSummary As you can see, the GNN is a powerful tool for solving graph-related problems. It can be used to solve a wide range of problems such as node classification, link prediction, and graph classification.\nRefernces Graph Neural Networks: A Review of Methods and Applications Graph Convolutional Networks Graph Attention Networks PyTorch Geometric Intro to graph neural networks (ML Tech Talks) Graph Neural Networks with PyG on Node Classification, Link Prediction, and Anomaly Detection ","permalink":"https://nutorbit.github.io/posts/graph_neural_network/","summary":"If you pay attention to everything around you, you will notice that graphs are everywhere. It is natural to represent many things as graphs. For example, a social network can be represented as a graph where the nodes are people and the edges are friendships. Molecules can be represented as graphs where the nodes are atoms and the edges are chemical bonds.\nTackle the problem of learning from graph structure with traditional methods requires a lot of domain knowledge.","title":"Graphs are everywhere"}]